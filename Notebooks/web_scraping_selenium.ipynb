{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Unscrapable\n",
    "\n",
    "Some sites are hard to scrape.\n",
    "\n",
    "Sometimes you get blocked.\n",
    "Sometimes the site is using a lot of fancy Javascript.\n",
    "\n",
    "We'll see a few examples of methods we can use as workarounds for the former and introduce the tool Selenium that lets us automate dynamic interactions with the browser, which can help with the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much is too much?\n",
    "\n",
    "Sites have `robots.txt` pages that give guidelines about what they want to allow webcrawlers to access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:05:12.265423Z",
     "start_time": "2019-04-15T18:05:11.717673Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://www.github.com/robots.txt'\n",
    "response  = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disallow: / means disallow everything (for all user-agents at the end that aren't covered earlier). Boxofficemojo is more accepting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:05:13.371493Z",
     "start_time": "2019-04-15T18:05:12.359473Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://www.boxofficemojo.com/robots.txt'\n",
    "response  = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very common for sites to block you if you send too many requests in a certain time period. Sometimes all it takes to evade this is well-designed pauses in your scraping. \n",
    "\n",
    "2 general ways:\n",
    "* pause after every request\n",
    "* pause after each n requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:05:19.500434Z",
     "start_time": "2019-04-15T18:05:13.481810Z"
    }
   },
   "outputs": [],
   "source": [
    "#every request\n",
    "import time\n",
    "\n",
    "page_list = ['page1','page2','page3']\n",
    "\n",
    "for page in page_list:\n",
    "    ### scrape a website\n",
    "    ### ...\n",
    "    print(page)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:05:19.526065Z",
     "start_time": "2019-04-15T18:05:19.511446Z"
    }
   },
   "outputs": [],
   "source": [
    "#every 200 requests\n",
    "import time\n",
    "\n",
    "page_list = ['page1','page2','page3','page4','page5','page6']\n",
    "\n",
    "for i, page in enumerate(page_list):\n",
    "    ### scrape a website\n",
    "    ### ...\n",
    "    print(page)\n",
    "    \n",
    "    if (i+1 % 200 == 0):\n",
    "        time.sleep(320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or better yet, add a random delay (more human-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:05:30.114862Z",
     "start_time": "2019-04-15T18:05:19.533187Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for page in page_list:\n",
    "    ### scrape a website\n",
    "    ### ...\n",
    "    print(page)\n",
    "    \n",
    "    time.sleep(.5+2*random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I make requests look like a real browser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:05:32.550138Z",
     "start_time": "2019-04-15T18:05:30.122006Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://www.reddit.com'\n",
    "\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "response  = requests.get(url, headers = user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a random user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:05:57.089823Z",
     "start_time": "2019-04-15T18:05:32.553759Z"
    }
   },
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = {'User-agent': ua.random}\n",
    "print(user_agent)\n",
    "\n",
    "response  = requests.get(url, headers = user_agent)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to Selenium!\n",
    "\n",
    "## What happens if I try to parse my gmail with `requests` and `BeautifulSoup`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:06:43.751969Z",
     "start_time": "2019-04-15T18:06:43.017978Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "gmail_url=\"https://mail.google.com\"\n",
    "soup=BeautifulSoup(requests.get(gmail_url).text, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is a tiny page. We get redirected. Soupifying this is useless, of course. Luckily, in this case we can see where we are sent to. In many of cases, you won't be so lucky. The page contents will be rendered by JavaScript by a browser, so just getting the source won't help you.\n",
    "\n",
    "Anyway, let's follow the redirection for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:06:48.261117Z",
     "start_time": "2019-04-15T18:06:47.756676Z"
    }
   },
   "outputs": [],
   "source": [
    "new_url = \"https://mail.google.com/mail\"\n",
    "\n",
    "# get method will navigate the requested url.. \n",
    "soup =BeautifulSoup(requests.get(new_url).text)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:06:49.067097Z",
     "start_time": "2019-04-15T18:06:49.061093Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.find(id='Email'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have hit the login page. We can't get to the emails without logging in ... i.e. we need to actually interact with the browser using Selenium!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromedriver_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:07:03.924781Z",
     "start_time": "2019-04-15T18:06:57.847339Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "#import chromedriver_binary\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome('/Users/adelweiss/Documents/Project/Project 2')\n",
    "driver.get(\"https://mail.google.com\")\n",
    "\n",
    "# Alternatives to Chrome: Firefox, PhantomJS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: how to include usernames and passwords\n",
    "\n",
    "We are going to have to enter a username  and password in order to log in. However, we **don't** want to have our password uploaded to Github for people to scrape! One solution to this is to use _environment variables_.\n",
    "\n",
    "In your directory, create a file called `.env` that has the following format:\n",
    "```bash\n",
    "USERNAME=\"your_username@gmail.com\"\n",
    "PASSWORD=\"your_password\"\n",
    "```\n",
    "DON'T ADD THIS FILE TO GITHUB!\n",
    "It is prudent to add a line `.env` to your `.gitignore`\n",
    "\n",
    "We add two commands to the top of the cell:\n",
    "```\n",
    "%load_ext dotenv  # allows us to use the %dotenv \"magic\" command\n",
    "%dotenv           # reads .env, and makes USERNAME and PASSWORD environment variables\n",
    "```\n",
    "We can now use `os.environ.get` to access the environment variables without having them appear in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:07:13.702273Z",
     "start_time": "2019-04-15T18:07:09.670197Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:48.218167Z",
     "start_time": "2019-01-23T20:24:48.153631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See notes about environment variables\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "EMAIL = os.environ.get('USERNAME')\n",
    "PASSWORD = os.environ.get('PASSWORD')\n",
    "\n",
    "# Show that this is working. Don't do this for PASSWORD!\n",
    "print(EMAIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill out username and password, hit enter to log in\n",
    "\n",
    "Now let use this to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:48.526510Z",
     "start_time": "2019-01-23T20:24:48.220146Z"
    }
   },
   "outputs": [],
   "source": [
    "username_form = driver.find_element_by_id(\"identifierId\")\n",
    "username_form.send_keys(EMAIL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:48.646827Z",
     "start_time": "2019-01-23T20:24:48.529641Z"
    }
   },
   "outputs": [],
   "source": [
    "username_form.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:26:17.099624Z",
     "start_time": "2019-01-23T20:26:16.978329Z"
    }
   },
   "outputs": [],
   "source": [
    "password_form=driver.find_element_by_name(\"password\") # note another approach\n",
    "password_form.send_keys(PASSWORD) # enter password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:26:21.116461Z",
     "start_time": "2019-01-23T20:26:21.027026Z"
    }
   },
   "outputs": [],
   "source": [
    "password_form.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click compose button to start a new email draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:26:34.425032Z",
     "start_time": "2019-01-23T20:26:29.150348Z"
    }
   },
   "outputs": [],
   "source": [
    "compose_button=driver.find_element_by_xpath('//div[text()=\"Compose\"]')\n",
    "compose_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a nice, friendly (optional) message to your (least?) favorite person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:28:31.651119Z",
     "start_time": "2019-01-23T20:28:31.322183Z"
    }
   },
   "outputs": [],
   "source": [
    "to_field = driver.find_element_by_name(\"to\")\n",
    "to_field.send_keys(\"email@gmail.com\") # enter recipient email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:28:37.194026Z",
     "start_time": "2019-01-23T20:28:36.897536Z"
    }
   },
   "outputs": [],
   "source": [
    "subject = driver.find_element_by_name(\"subjectbox\")\n",
    "subject.send_keys(\"This is an alert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:28:39.586376Z",
     "start_time": "2019-01-23T20:28:39.064163Z"
    }
   },
   "outputs": [],
   "source": [
    "message_body = driver.find_element_by_xpath(\"//div[@aria-label='Message Body']\")\n",
    "message_body.send_keys(\"Hello,\")\n",
    "message_body.send_keys([Keys.RETURN, Keys.RETURN])\n",
    "message_body.send_keys(\"I am a computer and I just became self aware!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Press the send button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:28:44.209370Z",
     "start_time": "2019-01-23T20:28:43.292555Z"
    }
   },
   "outputs": [],
   "source": [
    "send_button = driver.find_element_by_xpath(\"//div[contains(@aria-label, 'Send')]\")\n",
    "send_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Box Office Mojo with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:28:52.402164Z",
     "start_time": "2019-01-23T20:28:47.996675Z"
    }
   },
   "outputs": [],
   "source": [
    "matrix_url = \"http://www.boxofficemojo.com/movies/?id=matrix.htm\"\n",
    "driver.get(matrix_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:28:56.693004Z",
     "start_time": "2019-01-23T20:28:56.647800Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'contains' will find a match on the text, in this case return b tag\n",
    "gross_selector = '//font[contains(text(), \"Domestic\")]/b'\n",
    "print(driver.find_element_by_xpath(gross_selector).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:28:57.641747Z",
     "start_time": "2019-01-23T20:28:57.526278Z"
    }
   },
   "outputs": [],
   "source": [
    "# scraping genre\n",
    "genre_selector = '//a[contains(@href, \"/genres/chart/\")]/b'\n",
    "for genre_anchor in driver.find_elements_by_xpath(genre_selector):\n",
    "    print(genre_anchor.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:29:00.318500Z",
     "start_time": "2019-01-23T20:29:00.256792Z"
    }
   },
   "outputs": [],
   "source": [
    "inf_adjust_2000_selector = '//select[@name=\"ticketyr\"]/option[@value=\"2000\"]'\n",
    "driver.find_element_by_xpath(inf_adjust_2000_selector).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:29:08.121653Z",
     "start_time": "2019-01-23T20:29:00.907359Z"
    }
   },
   "outputs": [],
   "source": [
    "go_button = driver.find_element_by_name(\"Go\")\n",
    "go_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the page has changed; it's showing inflation adjusted numbers. We can grab the new, adjusted number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:29:09.166754Z",
     "start_time": "2019-01-23T20:29:09.127576Z"
    }
   },
   "outputs": [],
   "source": [
    "gross_selector = '//font[contains(text(), \"Domestic \")]/b'\n",
    "print(driver.find_element_by_xpath(gross_selector).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping IMDB with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:29:17.886628Z",
     "start_time": "2019-01-23T20:29:10.669271Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://www.imdb.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:49.039945Z",
     "start_time": "2019-01-23T20:23:57.802Z"
    }
   },
   "outputs": [],
   "source": [
    "query = driver.find_element_by_id(\"navbar-query\")\n",
    "query.send_keys(\"Julianne Moore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:49.042149Z",
     "start_time": "2019-01-23T20:23:57.806Z"
    }
   },
   "outputs": [],
   "source": [
    "query.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:49.052960Z",
     "start_time": "2019-01-23T20:23:57.813Z"
    }
   },
   "outputs": [],
   "source": [
    "name_selector = '//a[contains(text(), \"Julianne Moore\")]'\n",
    "driver.find_element_by_xpath(name_selector).click()\n",
    "current_url = driver.current_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing Selenium and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:49.054818Z",
     "start_time": "2019-01-23T20:23:57.820Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\"\"\"Could use requests then send page.text to bs4\n",
    "but Selenium actually stores the source as part of\n",
    "the Selenium driver object inside driver.page_source\n",
    "\n",
    "#import requests\n",
    "#page = requests.get(current_url)\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:49.056882Z",
     "start_time": "2019-01-23T20:23:57.834Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:49.058916Z",
     "start_time": "2019-01-23T20:23:57.867Z"
    }
   },
   "outputs": [],
   "source": [
    "len(soup.find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:24:49.062232Z",
     "start_time": "2019-01-23T20:23:57.873Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ad0c9a213c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: If a page is static, we can just use Beautiful Soup. If there is some dynamic component or interaction, we can then bring Selenium into the mix. Selenium can be used on its own or in conjunction with Beautiful Soup.\n",
    "\n",
    "*References:* \n",
    "\n",
    "Documentation on finding elements:\n",
    "- https://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "Xpath tutorial:\n",
    "-  https://www.w3schools.com/xml/xpath_syntax.asp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
